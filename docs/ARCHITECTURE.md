# Antigravity Studio 架构设计白皮书

## 1. 核心冲突与技术演进

### 1.1 剪映 (JianYing) 自动化瓶颈
在项目初期，我们尝试直接操作剪映草稿 JSON。虽然这能利用剪映丰富的素材库，但遇到了以下致命问题：
- **状态不透明**：剪映是一个 GUI 软件，AI 无法实时感知渲染后的画面状态。
- **协议闭塞**：草稿协议频繁更新且不可控，且无法在无头环境（Headless）下运行方案。

### 1.2 FFmpeg 的局限
尝试使用 FFmpeg 滤镜直接渲染时，遇到了视觉质量的瓶颈：
- **Jitter 问题**：FFmpeg 处理动态缩放（zoompan）时，由于浮点精度和缓存机制，画面会出现肉眼可见的位移抖动。
- **扩展性差**：在 Filtergraph 中实现复杂的排版和阴影效果极其痛苦。

## 2. 解决方案：Remotion 驱动的 AI 渲染架构

我们决定将整个渲染层迁移至 **Remotion**，构建一个基于 React 的视频生成器。

### 2.1 架构模型：双态分离
- **逻辑态 (Python/FastAPI/MCP)**：负责“思考”。AI Agent 在这里分析文本、检索素材、规划时间轴，并下发 JSON 指令。
- **表现态 (React/Remotion)**：负责“执行”。根据 JSON 指令实时渲染每一帧。由于是浏览器驱动，我们可以利用现代 Web 的所有视觉能力（CSS, WebGL, Lottie）。

### 2.2 三大核心能力
1. **亚像素精度 (Sub-pixel Precision)**：利用 Chromium 的 GPU 加速和 React 的声明式动画，画面即使在 0.1% 的极慢缩放速度下依然平滑如丝。
2. **并行渲染 (Parallelized Sequence)**：利用 Remotion 的分片导出技术，将视频拆分为 N 个片段并发渲染，彻底释放多核 CPU 的潜力。
3. **数据桥接 (The Data Bridge)**：通过 WebSocket 实现逻辑端与表现端的双向通信。AI 可以实时查询当前视频的某一帧画面，并即时修改。

## 3. 未来的方向：AI 本地渲染
我们将逐步弱化“导出到剪映”的功能，转而强化“Antigravity 自研导出”。目标是实现一个无需用户干预、从创意到 4K 视频直出的自动化工厂。
